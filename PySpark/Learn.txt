PySpark is a Python API for Apache Spark.

What is PySpark?
PySpark is the Python library for Apaxhe Spark, designed for Large-Scale Data processing. It 
provides a high-level API for distributed dat processing, allowing you to leverage the power 
of Sparks's distributed computing framework from Python.

Installation:

1. Install Spark:
- Download Spark from Apache Spark website or use a package manager.
- Ensure Java is installed on your system since Spark runs on the Java Virtual Machine (JVM)

# Example using Homebrew on macOS:
brew install apache-spark

Set Environment Variables:
Add SPARK_HOME to your environment variables. 
Add $SPARK_HOME/bin to your PATH for easier access to Spark binaries.

Set Environment Variables:
Add SPARK_HOME to your environment variables. 
Add $SPARK_HOME/bin to your PATH for easier access to Spark binaries.

Install PySpark:
You can install PySpark using pip:
pip install pyspark

Basic PySpark Operations
Let's go through some basic operations:

Initialize Spark Session


Basic PySpark Operations
Let's go through some basic operations:


from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("PySparkTutorial") \
    .getOrCreate()

